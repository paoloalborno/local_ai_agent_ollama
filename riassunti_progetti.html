<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Riassunti dei Progetti</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <style>
        .summary-container {
            margin-bottom: 2rem;
        }
        .summary-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            cursor: pointer;
            padding: 1rem;
            background-color: #f0f0f0;
            border-radius: 8px;
        }
        .summary-content {
            display: none;
            padding: 1rem;
            border: 1px solid #ddd;
            border-top: none;
            border-radius: 0 0 8px 8px;
        }
        .summary-content pre {
            white-space: pre-wrap;
            word-wrap: break-word;
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 1em;
            border-radius: 5px;
        }
        .text-justify {
            text-align: justify;
        }
        h4 {
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
            border-bottom: 1px solid #ddd;
            padding-bottom: 0.5rem;
        }
    </style>
</head>
<body>
    <div id="header-placeholder"></div>

    <main class="container" style="padding-top: 120px;">
        <section class="page-section">
            <h1 class="section-title">Analisi del Repository: Agente AI Locale</h1>
            <div class="summary-container">
                <div class="summary-header" id="ollama-summary-header">
                    <h2>Panoramica del Progetto e Architettura</h2>
                    <i class="fas fa-chevron-down"></i>
                </div>
                <div class="summary-content" id="ollama-summary-content">
                    <div class="text-justify">
                        <p>
                            Questo progetto implementa un agente di intelligenza artificiale interamente locale, progettato per analizzare le recensioni dei prodotti. L'architettura è costruita per garantire la privacy dei dati, eliminare la dipendenza da API di terze parti e assicurare una bassa latenza. Il sistema sfrutta <strong>Ollama</strong> per l'esecuzione di modelli linguistici di grandi dimensioni (LLM) e <strong>ChromaDB</strong> per la ricerca semantica basata su vettori.
                        </p>
                        <p>
                            Il protocollo di comunicazione personalizzato, <strong>Model Context Protocol (MCP)</strong>, standardizza l'interazione tra un client e un server, permettendo di esporre gli strumenti di analisi come funzioni richiamabili tramite JSON-RPC. Questo design modulare è una best practice che favorisce la manutenibilità e la scalabilità del sistema.
                        </p>
                        <h4>Architettura Generale</h4>
                        <p>
                            Il sistema è diviso in componenti chiave:
                            <ul>
                                <li><strong>Server MCP (`mcp_server.py`):</strong> Il backend che espone gli strumenti AI tramite l'MCP.</li>
                                <li><strong>Client MCP (`mcp_client.py`):</strong> Un'interfaccia a riga di comando per interagire con il server.</li>
                                <li><strong>Agente (`ollama_agent.py`):</strong> L'orchestratore che esegue una sequenza di strumenti per rispondere a query complesse.</li>
                                <li><strong>Strumenti (`tools.py`):</strong> Le singole funzioni che eseguono compiti specifici come l'estrazione di keyword o la sintesi di testi.</li>
                                <li><strong>Vector Store (`vector.py`):</strong> Il gestore del database vettoriale che si occupa della memorizzazione e del recupero delle recensioni.</li>
                            </ul>
                        </p>

                        <h4>`mcp_server.py` - Il Cuore del Backend</h4>
                        <p>
                            Questo file è il punto di ingresso del server. Utilizza la libreria `mcp.server` per creare un server JSON-RPC. La funzione `initialize_system` è fondamentale perché carica il modello LLM, inizializza il database vettoriale e prepara gli strumenti per l'uso.
                        </p>
                        <pre><code class="language-python">
# mcp_server.py
def initialize_system(model_name: str = "llama3.2:latest", k: int = 5) -> bool:
    """Initialize all components needed for the MCP server"""
    global llm, vector_store, retriever, agent_tools, ollama_agent
    try:
        # Load LLM
        print(f"Loading model: {model_name}", file=sys.stderr)
        llm = OllamaLLM(model=model_name)
        # ... (initialization of vector_store, retriever, etc.)
        return True
    except Exception as e:
        # ... (error handling)
        return False
                        </code></pre>
                        <p>
                            Il decoratore `@server.call_tool()` espone una funzione come uno strumento richiamabile dal client. Questo è un eccellente esempio di come creare un'API strutturata per un agente AI.
                        </p>
                        <pre><code class="language-python">
# mcp_server.py
@server.call_tool()
async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> List[types.TextContent]:
    try:
        if not agent_tools:
            return [types.TextContent(text=json.dumps({"error": "Agent tools not initialized"}))]

        if name == "extract_important_keywords":
            # ... logic to call the tool
            result = agent_tools.extract_important_keywords(arguments["user_query"])
            return [types.TextContent(text=json.dumps(result, ensure_ascii=False))]
        # ... (other tools)
    except Exception as e:
        # ... (error handling)
                        </code></pre>

                        <h4>`mcp_client.py` - Interfaccia Utente</h4>
                        <p>
                            Il client è un'applicazione a riga di comando asincrona che comunica con il server MCP. Avvia il server come un sottoprocesso e invia richieste JSON-RPC per eseguire i comandi. La lezione qui è la separazione netta tra l'interfaccia utente e la logica del server, che permette di sviluppare e testare i due componenti in modo indipendente.
                        </p>
                        <pre><code class="language-python">
# mcp_client.py
async def call_tool(self, name, arguments):
    timeout = 60.0 if name == "agent" else 10.0
    response = await self._send_request("tools/call", name, {
        "name": name,
        "arguments": arguments
    }, timeout)
    if response and "result" in response:
        return response["result"]
    return None
                        </code></pre>

                        <h4>`ollama_agent.py` - L'Orchestratore</h4>
                        <p>
                            Questo file definisce l'agente che orchestra l'esecuzione sequenziale degli strumenti. Il metodo `run_sequenced` è un chiaro esempio di "chain-of-thought" (catena di pensiero) implementata programmaticamente. L'agente esegue una serie di passaggi logici (estrazione, recupero, sintesi) per produrre un risultato completo.
                        </p>
                        <pre><code class="language-python">
# ollama_agent.py
def run_sequenced(self, user_query: str) -> str:
    """Execute tools in a fixed sequence and return JSON result"""
    try:
        # Step 1: Extract keywords
        keywords = self.agent_tools.extract_important_keywords(user_query)
        # Step 2: Retrieve reviews
        reviews = self.agent_tools.retrieve_useful_reviews(keywords)
        # Step 3: Summarize reviews
        summary = self.agent_tools.summarize_reviews(reviews)
        # ... (and so on)
        result = { "query": user_query, "keywords": keywords, ... }
        return json.dumps(result, indent=2, ensure_ascii=False)
    except Exception as e:
        # ... (error handling)
                        </code></pre>

                        <h4>`tools.py` - Gli Strumenti Atomici</h4>
                        <p>
                            Qui risiede la logica di ogni singolo strumento. Una best practice fondamentale mostrata in questo file è l'uso di prompt ben definiti e specifici per guidare il modello LLM. Ogni prompt è una stringa formattabile che definisce chiaramente il compito che il modello deve eseguire, riducendo l'ambiguità e migliorando la qualità dei risultati.
                        </p>
                        <pre><code class="language-python">
# tools.py
self.prompt_keywords = (
    "Given the following user query your goal is to extract the most important keywords on order to retrieve related reviews via RAG.\\n"
    "Add only very close synonyms. "
    "User query: {user_query}\\n"
    "Return ONLY a comma-separated list of keywords, no explanations."
)
# ...
def extract_important_keywords(self, user_query: str) -> List[str]:
    prompt = self.prompt_keywords.format(user_query=user_query)
    try:
        response = self.llm.invoke(prompt)
        # ... (parsing logic)
    except Exception as e:
        # ...
                        </code></pre>

                        <h4>`vector.py` - La Memoria Semantica</h4>
                        <p>
                            Questo file gestisce la creazione e l'interazione con il database vettoriale ChromaDB. La classe `ReviewsVectorStore` astrae la complessità del caricamento dei dati da un CSV, della creazione degli embedding e dell'inizializzazione del database. La funzione `get_retriever` è particolarmente importante perché fornisce un'interfaccia standard (un oggetto `BaseRetriever` di LangChain) per eseguire la ricerca semantica.
                        </p>
                        <pre><code class="language-python">
# vector.py
class ReviewsVectorStore:
    def __init__(self, csv_file_path: str = "reviews.csv", ...):
        # ...
        self.vector_store = Chroma(
            client=chromadb.PersistentClient(path=db_location),
            collection_name=collection_name,
            embedding_function=self.embeddings
        )

    def get_retriever(self, k: int = 10):
        k = max(1, min(50, int(k)))
        return self.vector_store.as_retriever(search_kwargs={"k": k})
                        </code></pre>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <div id="footer-placeholder"></div>
    <script src="https://www.gstatic.com/firebasejs/10.5.0/firebase-app-compat.js"></script>
    <script src="https://www.gstatic.com/firebasejs/10.5.0/firebase-auth-compat.js"></script>
    <script src="../assets/js/translations.js"></script>
    <script type="module" src="../assets/js/main.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const header = document.getElementById('ollama-summary-header');
            const content = document.getElementById('ollama-summary-content');
            const icon = header.querySelector('i');

            header.addEventListener('click', () => {
                const isHidden = content.style.display === 'none' || content.style.display === '';
                content.style.display = isHidden ? 'block' : 'none';
                icon.classList.toggle('fa-chevron-down', !isHidden);
                icon.classList.toggle('fa-chevron-up', isHidden);
            });
        });
    </script>
</body>
</html>